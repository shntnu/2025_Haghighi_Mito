#!/bin/bash
#
# download_data.sh - Download required data from S3 for mitochondrial morphology analysis
#
# DESCRIPTION:
#   Downloads ~4.52 GB of data from S3 bucket to local data/external/mito_project/ directory.
#   This includes metadata, per-site aggregated profiles, orthogonal feature lists, and
#   baseline virtual screen results needed for the analysis notebooks.
#
# USAGE:
#   bash scripts/download_data.sh
#
# REQUIREMENTS:
#   - AWS CLI installed and configured
#   - Access to s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/
#   - ~5 GB free disk space
#
# DOWNLOADS:
#   1. metadata/ (1.74 GB) - JUMP, LINCS, TA-ORF, CDRP metadata files
#   2. per_site_aggregated_profiles_newpattern_2/ (2.69 GB) - Pre-aggregated profiles for 6 datasets
#   3. results/target_pattern_orth_features_lists/ (9.8 KB) - Orthogonal feature CSVs
#   4. results/virtual_screen/ (~90 MB) - Baseline virtual screen results from July 2024
#
# OUTPUT:
#   data/external/mito_project/workspace/
#
# NOTES:
#   - Excludes metadata/preprocessed/ (generated by notebook)
#   - Downloads baseline virtual_screen results for comparison/validation
#   - Locally-regenerated results will use _REGEN suffix (won't overwrite baseline)
#   - Prompts for confirmation before downloading
#   - Exits on any error (set -e)
#

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

S3_BASE="s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace"
LOCAL_BASE="data/external/mito_project/workspace"

echo "========================================"
echo "S3 Data Download Script"
echo "========================================"
echo ""
echo "${YELLOW}This will download ~4.52 GB of data${NC}"
echo ""
read -p "Continue? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]
then
    echo "${RED}Download cancelled.${NC}"
    exit 1
fi

echo ""
echo "${GREEN}Creating local directory structure...${NC}"
mkdir -p "$LOCAL_BASE/metadata"
mkdir -p "$LOCAL_BASE/per_site_aggregated_profiles_newpattern_2"
mkdir -p "$LOCAL_BASE/results/target_pattern_orth_features_lists"
mkdir -p "$LOCAL_BASE/metadata/preprocessed"
mkdir -p "$LOCAL_BASE/results/virtual_screen"

echo "${BLUE}✓${NC} Directories created"
echo ""

# Download 1: metadata
echo "========================================"
echo "${YELLOW}[1/4] Downloading metadata/ (1.74 GB)${NC}"
echo "========================================"
aws s3 sync \
  "$S3_BASE/metadata/" \
  "$LOCAL_BASE/metadata/" \
  --exclude "preprocessed/*"

if [ $? -eq 0 ]; then
    echo "${GREEN}✓ metadata/ download complete${NC}"
else
    echo "${RED}✗ metadata/ download failed${NC}"
    exit 1
fi
echo ""

# Download 2: per-site profiles
echo "========================================"
echo "${YELLOW}[2/4] Downloading per_site_aggregated_profiles_newpattern_2/ (2.69 GB)${NC}"
echo "========================================"
aws s3 sync \
  "$S3_BASE/per_site_aggregated_profiles_newpattern_2/" \
  "$LOCAL_BASE/per_site_aggregated_profiles_newpattern_2/"

if [ $? -eq 0 ]; then
    echo "${GREEN}✓ per_site_aggregated_profiles_newpattern_2/ download complete${NC}"
else
    echo "${RED}✗ per_site_aggregated_profiles_newpattern_2/ download failed${NC}"
    exit 1
fi
echo ""

# Download 3: orthogonal features lists
echo "========================================"
echo "${YELLOW}[3/4] Downloading results/target_pattern_orth_features_lists/ (9.8 KB)${NC}"
echo "========================================"
aws s3 sync \
  "$S3_BASE/results/target_pattern_orth_features_lists/" \
  "$LOCAL_BASE/results/target_pattern_orth_features_lists/"

if [ $? -eq 0 ]; then
    echo "${GREEN}✓ results/target_pattern_orth_features_lists/ download complete${NC}"
else
    echo "${RED}✗ results/target_pattern_orth_features_lists/ download failed${NC}"
    exit 1
fi
echo ""

# Download 4: baseline virtual screen results
echo "========================================"
echo "${YELLOW}[4/4] Downloading results/virtual_screen/ (~90 MB - baseline results)${NC}"
echo "========================================"
aws s3 sync \
  "$S3_BASE/results/virtual_screen/" \
  "$LOCAL_BASE/results/virtual_screen/"

if [ $? -eq 0 ]; then
    echo "${GREEN}✓ results/virtual_screen/ download complete${NC}"
else
    echo "${RED}✗ results/virtual_screen/ download failed${NC}"
    exit 1
fi
echo ""

# Summary
echo "========================================"
echo "${GREEN}Download Complete!${NC}"
echo "========================================"
echo ""
echo "Downloaded to: $LOCAL_BASE"
echo ""
echo "Directory structure:"
echo "  ${BLUE}metadata/${NC} - Dataset metadata files"
echo "  ${BLUE}per_site_aggregated_profiles_newpattern_2/${NC} - Pre-aggregated profiles"
echo "  ${BLUE}results/target_pattern_orth_features_lists/${NC} - Feature lists"
echo "  ${BLUE}results/virtual_screen/${NC} - Baseline virtual screen results (July 2024)"
echo ""
echo "Created for outputs:"
echo "  ${BLUE}metadata/preprocessed/${NC} - Will be generated by notebook"
echo "  ${BLUE}results/virtual_screen/*_REGEN.csv${NC} - Locally-regenerated results (won't overwrite baseline)"
echo ""
echo "${YELLOW}Next steps:${NC}"
echo "  1. The notebook path is already configured to use this data"
echo "  2. Run the notebook: notebooks/2.0-mh-virtual-screen.py"
echo ""
