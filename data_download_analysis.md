# Data Download Analysis for All Analysis Notebooks

## Summary

**Total data needed from S3: ~5.1 GB** (not 57 GB!)

*Updated Oct 2025: Now includes baseline virtual_screen results (~90 MB) for comparison*

This analysis covers data requirements for all notebooks in the pipeline:

- **1.0-mh-feat-importance.py** - Feature importance analysis
- **2.0-mh-virtual-screen.py** - Virtual screening (primary pipeline)
- **2.1-mh-set-enrichment-analysis.py** - Gene set enrichment analysis
- **2.2-mh-check-vs-lists.py** - Validation and comparison

Most notebooks **WRITE** to the `results/` directory rather than reading from it, so we don't need most of that 52 GB.

---

## Detailed Breakdown

### 1. **metadata/** - **2.1 GB** ✅ DOWNLOAD

**43 files total**

Files the notebook reads:

- `JUMP-ORF/ORF_list.tsv` (~1 MB)
- `lincs/DrugRepurposing_Metadata.csv` (~26 MB)
- `LINCS_meta.csv` (~15 MB)
- `JUMP/plate.csv.gz`, `well.csv.gz`, `compound.csv.gz`, `orf.csv.gz`, `crispr.csv.gz` (~various)
- `TA-ORF/replicate_level_cp_normalized.csv.gz` (~varies)
- `CDRP_meta.csv` (~26 MB)

Files the notebook writes (created locally):

- `preprocessed/annot_*.csv` (generated by cells at lines 124, 143, 173, 192, 198, 231)

**Action:** Download entire metadata directory except `preprocessed/`

---

### 2. **per_site_aggregated_profiles_newpattern_2/** - **2.7 GB** ✅ DOWNLOAD

**150 files across 6 dataset subdirectories**

The notebook **reads** from this directory at line 1087:

```python
per_site_df_b = pd.read_csv(fileNameToSave)
```

Subdirectories:

- `CDRP/` - batch files like `*_site_agg_profiles.csv.gz`
- `jump_compound/`
- `jump_crispr/`
- `jump_orf/`
- `lincs/`
- `taorf/`

**Action:** Download entire directory

---

### 3. **results/target_pattern_orth_features_lists/** - **~10 KB** ✅ DOWNLOAD

**8 small CSV files**

The notebook **reads** from this directory at lines 967, 970, 986, 991, 1006, 1048, 1064:

```python
uncorr_feats_condese = pd.read_csv(
    save_results_dir + "target_pattern_orth_features_lists/fibroblast_derived.csv"
)["orth_fs"].tolist()
```

Files needed:

- `CDRP.csv` (765 B)
- `fibroblast_derived.csv` (757 B) ← **Most commonly used**
- `jump_crispr.csv` (495 B)
- `jump_orf.csv` (1.6 KB)
- `lincs.csv` (4.0 KB)
- `lincs_2.csv` (1.5 KB)
- `taorf.csv` (557 B)

**Action:** Download only this subdirectory

---

### 4. **results/virtual_screen/** - **~90 MB** ✅ DOWNLOAD (Updated Oct 2025)

**Status Update:** Initially marked ❌ (outputs only), but investigation revealed these are needed as baseline/reference.

**26 CSV files - BASELINE results from July 2024**

The notebook **writes** to this directory at line 1410-1416, BUT we also **download** the S3 versions for:

1. **Comparison**: Verify reproducibility of analysis pipeline
2. **Baseline**: Reference results for validation against regenerated outputs
3. **Debugging**: Identify when/why analysis results diverged

**Key Files (most recent `_aug_070624` versions):**
- `CDRP_results_pattern_aug_070624.csv` (9.4 MB)
- `jump_compound_results_pattern_aug_070624.csv` (47 MB)
- `jump_crispr_results_pattern_aug_070624.csv` (2.1 MB)
- `jump_orf_results_pattern_aug_070624.csv` (4.2 MB)
- `lincs_results_pattern_aug_070624.csv` (3.7 MB)
- `taorf_results_pattern_aug_070624.csv` (93 KB)

**Naming Convention:**
- **S3 baseline**: Original names (e.g., `lincs_results_pattern_aug_070624.csv`)
- **Locally regenerated**: Add `_REGEN` suffix (e.g., `lincs_results_pattern_aug_070624_REGEN.csv`)

This allows both versions to coexist for side-by-side comparison.

**Action:** Download entire directory for baseline comparison

---

### 5. **results/** (rest of 52 GB) - ❌ NOT NEEDED

**1,900+ files in other subdirectories**

Other subdirectories in results/:

- `ORA/`
- `TopLincsCompoundsAnalysis/`
- `reverse_phenotype_strength/`
- `slope_subjects/`
- etc.

**None of these are referenced in the notebook.**

**Action:** Do NOT download

---

## Recommended Download Commands

### Option 1: Download everything needed (~5.1 GB) - Use the script!

```bash
# Automated download with the provided script (RECOMMENDED)
bash scripts/download_data.sh
```

This downloads:
1. metadata/ (1.74 GB)
2. per_site_aggregated_profiles_newpattern_2/ (2.69 GB)
3. results/target_pattern_orth_features_lists/ (9.8 KB)
4. results/virtual_screen/ (~90 MB - baseline from July 2024)

### Option 2: Manual download (if needed)

```bash
# Create local directory structure
mkdir -p data/external/mito_project/workspace

# Download metadata (1.74 GB) - exclude preprocessed since notebook generates it
aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/metadata/ \
  data/external/mito_project/workspace/metadata/ \
  --exclude "preprocessed/*"

# Download per-site profiles (2.69 GB)
aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/per_site_aggregated_profiles_newpattern_2/ \
  data/external/mito_project/workspace/per_site_aggregated_profiles_newpattern_2/

# Download orthogonal features lists (~10 KB)
aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/results/target_pattern_orth_features_lists/ \
  data/external/mito_project/workspace/results/target_pattern_orth_features_lists/

# Download baseline virtual screen results (~90 MB)
aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/results/virtual_screen/ \
  data/external/mito_project/workspace/results/virtual_screen/

# Create output directories
mkdir -p data/external/mito_project/workspace/metadata/preprocessed
```

### Option 2: Dry run to verify sizes

```bash
# Dry run - see what would be downloaded
aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/metadata/ \
  data/external/mito_project/workspace/metadata/ \
  --exclude "preprocessed/*" \
  --dryrun

aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/per_site_aggregated_profiles_newpattern_2/ \
  data/external/mito_project/workspace/per_site_aggregated_profiles_newpattern_2/ \
  --dryrun

aws s3 sync \
  s3://imaging-platform/projects/2016_08_01_RadialMitochondriaDistribution_donna/workspace/results/target_pattern_orth_features_lists/ \
  data/external/mito_project/workspace/results/target_pattern_orth_features_lists/ \
  --dryrun
```

---

## Notebook Modifications Needed

After downloading, update the notebook path at line 100:

**Before:**

```python
mito_project_root_dir = (
    home_path + "bucket/projects/2016_08_01_RadialMitochondriaDistribution_donna/"
)
```

**After:**

```python
mito_project_root_dir = "data/external/mito_project/"
```

---

## Data Flow Summary

### Inputs (READ from S3 → download needed)

1. ✅ `metadata/` - 1.74 GB
2. ✅ `per_site_aggregated_profiles_newpattern_2/` - 2.69 GB
3. ✅ `results/target_pattern_orth_features_lists/` - 9.8 KB
4. ✅ `results/virtual_screen/` - ~90 MB (baseline for comparison)

### Outputs (WRITTEN by notebook → create locally)

1. `metadata/preprocessed/annot_*.csv`
2. `results/virtual_screen/*_results_pattern_aug_070624_REGEN.csv` (regenerated, coexists with baseline)
3. `results/target_pattern_orth_features_lists/*.csv` (some cells write to this too)

**Total download: ~4.52 GB**

---

## Notebook-Specific Data Requirements

### 1.0-mh-feat-importance.py

**Purpose**: Feature importance analysis for mitochondrial morphology measurements

**Data Reads**:
- Uses `rootDir = bucket/projects/2016_08_01_RadialMitochondriaDistribution_donna/`
- Accesses same directories as 2.0 notebook (metadata, profiles)

**Data Writes**:
- Feature importance results (outputs only)

**Status**: ✅ Covered by main download (metadata + per_site_profiles)

---

### 2.0-mh-virtual-screen.py

**Purpose**: Primary virtual screening pipeline

**Data Reads**:
- `metadata/` - All dataset metadata (JUMP, LINCS, CDRP, TA-ORF)
- `per_site_aggregated_profiles_newpattern_2/` - Pre-aggregated profiles
- `results/target_pattern_orth_features_lists/` - Orthogonal feature lists

**Data Writes**:
- `metadata/preprocessed/annot_*.csv`
- `results/virtual_screen/*_results_pattern_aug_070624.csv`

**Status**: ✅ Primary focus of download scripts

---

### 2.1-mh-set-enrichment-analysis.py

**Purpose**: Gene set enrichment analysis (GSEA) on virtual screen results

**Data Reads FROM S3**:
- `results/virtual_screen/*_results_pattern_aug_070624.csv` (written by 2.0 notebook)
- `metadata/CompoundClusters08202104.xlsx`

**Data Reads FROM LOCAL** (`data/external/`):
- `KEGG_2021_Human_table.txt` (18 KB) - Already in repo ✅
- `WikiPathways_2024_Human_table.txt` (20 KB) - Already in repo ✅

**Data Writes**:
- Enrichment analysis results (outputs only)

**Status**: ✅ Requires running 2.0 notebook first to generate virtual_screen results

**Note**: Uses `blitzgsea` library for enrichment analysis

---

### 2.2-mh-check-vs-lists.py

**Purpose**: Validation and comparison of virtual screen hit lists

**Data Reads**:
- `results/virtual_screen/*_results_pattern_aug_070624.csv` (from 2.0 notebook)
- `results/target_pattern_orth_features_lists/` - Orthogonal feature lists
- `metadata/` - For validation cross-references

**Data Writes**:
- Comparison and validation results (outputs only)

**Status**: ✅ Requires running 2.0 notebook first

---

## Data Dependencies Flow

```
S3 Download (scripts/download_data.sh)
  ↓
  ├─ metadata/
  ├─ per_site_aggregated_profiles_newpattern_2/
  └─ results/target_pattern_orth_features_lists/

Notebook Execution Order:
  ↓
1.0-mh-feat-importance.py (optional - feature exploration)
  ↓
2.0-mh-virtual-screen.py (required - generates virtual_screen results)
  ↓
  ├─ 2.1-mh-set-enrichment-analysis.py (reads virtual_screen outputs)
  └─ 2.2-mh-check-vs-lists.py (reads virtual_screen outputs)
```

---

## Local Reference Data (Already in Repo)

These files are already committed in `data/external/` and do NOT need downloading:

1. **KEGG_2021_Human_table.txt** (18 KB)
   - Used by: 2.1-mh-set-enrichment-analysis.py
   - Purpose: KEGG pathway annotations for enrichment analysis

2. **WikiPathways_2024_Human_table.txt** (20 KB)
   - Used by: 2.1-mh-set-enrichment-analysis.py
   - Purpose: WikiPathways annotations for enrichment analysis

---

## Complete Download Requirements Summary

### From S3 (via scripts/download_data.sh):
- metadata/ (1.74 GB)
- per_site_aggregated_profiles_newpattern_2/ (2.69 GB)
- results/target_pattern_orth_features_lists/ (9.8 KB)
- results/virtual_screen/ (~90 MB) - baseline from July 2024

### From Local Repo (already committed):
- data/external/KEGG_2021_Human_table.txt (18 KB)
- data/external/WikiPathways_2024_Human_table.txt (20 KB)

### Generated by Notebooks (do NOT download):
- metadata/preprocessed/annot_*.csv
- results/virtual_screen/*_results_pattern_aug_070624_REGEN.csv (locally regenerated)

**Total from S3: ~4.52 GB**
**Total from repo: ~38 KB**
**Grand total: ~4.52 GB**
